// Command photocamera-archiver creates an archive of a Static Certificate
// Transparency log, by compressing tiles into zip files, each containing a
// subtree of up to 16,777,216 entries (65,536 level -1 and 0 tiles, 256 level 1
// tiles, and 1 level 2 tile). The checkpoint, JSON metadata, issuers, level 3+
// tiles, and partial tiles on the right edge are included in every zip file.
// The zip files are stored at archive/<N>.zip.
package main

import (
	"archive/zip"
	"context"
	"crypto/sha256"
	"crypto/x509"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"io/fs"
	"log/slog"
	"os"
	"os/signal"
	"slices"
	"strings"

	"filippo.io/sunlight"
	"filippo.io/torchwood"
	"geomys.org/ct-archive/internal/stdlog"
	"github.com/schollz/progressbar/v3"
	"golang.org/x/mod/sumdb/note"
	"golang.org/x/mod/sumdb/tlog"
)

const README = `This is an archive of a Certificate Transparency log, stored in the
c2sp.org/static-ct-api@v1.0.0 format, although if this log was originally served
through RFC 6962 APIs, leaves might miss the LeafIndex extension.

The log is split over multiple zip files. Archive <N>.zip contains the tiles
at levels -1 (the data entries), 0, 1, and 2 belonging to subtree

    [ 256×256×256×N, min(256×256×256×(N+1), TreeSize) )

In other words, each zip file contains one level 2 tile, and all those below it.

Every zip file also contains the following files:

  - README.txt — this file

  - checkpoint — a Signed Tree Head in c2sp.org/tlog-checkpoint format

  - log.v3.json — a JSON specification of the log, including its log ID, public
                  key, and original URL

  - issuer/* — the X.509 chain issuers for the whole log

  - tile/{3,4}/* — higher-level tiles, including the hash of other level 2
                   tiles not included in this archive

  - tile/{0,1,2}/*.p/* — partial tiles on the right edge of the tree, if any,
                         necessary to compute the tree head

This archive was generated by geomys.org/ct-archive/cmd/photocamera-archiver.
`

const archiveWidth = 256 * 256 * 256

func main() {
	logger := slog.New(stdlog.Handler)

	ctx, stop := signal.NotifyContext(context.Background(), os.Interrupt)
	defer stop()

	root, err := os.OpenRoot(".")
	if err != nil {
		fatalError(logger, "failed to open local directory", "err", err)
	}
	if err := root.MkdirAll("archive", 0o755); err != nil {
		fatalError(logger, "failed to create archive directory", "err", err)
	}
	tr := localTileReader{root.FS()}

	var logInfo struct {
		Description   string `json:"description"`
		Key           []byte `json:"key"`
		URL           string `json:"url"`
		SubmissionURL string `json:"submission_url"`
	}
	logBytes, err := root.ReadFile("log.v3.json")
	if err != nil {
		fatalError(logger, "failed to read log info", "err", err)
	}
	if err := json.Unmarshal(logBytes, &logInfo); err != nil {
		fatalError(logger, "failed to parse log info", "err", err)
	}
	key, err := x509.ParsePKIXPublicKey(logInfo.Key)
	if err != nil {
		fatalError(logger, "failed to parse log public key", "err", err)
	}
	url := logInfo.URL
	if url == "" {
		url = logInfo.SubmissionURL
	}
	origin := strings.TrimPrefix(url, "https://")
	origin = strings.TrimSuffix(origin, "/")
	v, err := sunlight.NewRFC6962Verifier(origin, key)
	if err != nil {
		fatalError(logger, "failed to construct log verifier", "err", err)
	}
	checkpointBytes, err := root.ReadFile("checkpoint")
	if err != nil {
		fatalError(logger, "failed to read checkpoint", "err", err)
	}
	n, err := note.Open(checkpointBytes, note.VerifierList(v))
	if err != nil {
		fatalError(logger, "failed to verify checkpoint", "err", err)
	}
	c, err := torchwood.ParseCheckpoint(n.Text)
	if err != nil {
		fatalError(logger, "failed to parse checkpoint", "err", err)
	}
	logger.Info("loaded checkpoint", "tree_size", c.N, "root_hash", c.Hash)

	var globalTiles []tlog.Tile
	for L := 5; L >= 0; L-- {
		levelHashes := c.N / int64(1<<(sunlight.TileHeight*L))
		if L >= 3 {
			// All level 3+ tiles.
			for N := int64(0); (N * 256) < levelHashes; N++ {
				W := int(min(256, levelHashes-(N*256)))
				tile := tlog.Tile{H: sunlight.TileHeight, L: L, N: N, W: W}
				globalTiles = append(globalTiles, tile)
			}
		} else {
			// Only the partial tile on the right edge, if any.
			if W := int(levelHashes % 256); W != 0 {
				N := levelHashes / 256
				tile := tlog.Tile{H: sunlight.TileHeight, L: L, N: N, W: W}
				globalTiles = append(globalTiles, tile)
			}
		}
	}

	hr := torchwood.TileHashReaderWithContext(ctx, c.Tree, tr)

	for n := int64(0); n < c.N; n += archiveWidth {
		i := n / archiveWidth
		if i >= 1000 {
			fatalError(logger, "cannot archive more than 1000 zip files")
		}
		name := fmt.Sprintf("archive/%03d.zip", i)
		subtree := min(archiveWidth, c.N-n)
		if _, err := os.Stat(name); err == nil {
			logger.Info("zip file exists", "name", name, "start", n, "end", n+subtree)
			continue
		}
		logger.Info("processing subtree", "name", name, "start", n, "end", n+subtree)
		f, err := root.Create(name + ".tmp")
		if err != nil {
			fatalError(logger, "failed to create zip file", "name", name, "err", err)
		}
		w := zip.NewWriter(f)
		comment := fmt.Sprintf("%s archive (%d of %d, generated by photocamera-archiver)",
			c.Origin, i+1, (c.N+archiveWidth-1)/archiveWidth)
		if err := w.SetComment(comment); err != nil {
			fatalError(logger, "failed to set zip comment", "name", name, "err", err)
		}

		// Store README, checkpoint, and log info. Uncompressed, since they're small.
		if err := storeMetadataFile(w, "README.txt", []byte(README)); err != nil {
			fatalError(logger, "failed to write README", "name", name, "err", err)
		}
		if err := storeMetadataFile(w, "checkpoint", checkpointBytes); err != nil {
			fatalError(logger, "failed to write checkpoint", "name", name, "err", err)
		}
		if err := storeMetadataFile(w, "log.v3.json", logBytes); err != nil {
			fatalError(logger, "failed to write log info", "name", name, "err", err)
		}

		// Store issuers.
		issuersDirEntries, err := fs.ReadDir(root.FS(), "issuer")
		if err != nil {
			fatalError(logger, "failed to read issuers directory", "err", err)
		}
		for _, de := range issuersDirEntries {
			if de.IsDir() {
				continue
			}
			name := "issuer/" + de.Name()
			data, err := root.ReadFile(name)
			if err != nil {
				fatalError(logger, "failed to read issuer file", "file", name, "err", err)
			}
			sum := sha256.Sum256(data)
			if hex.EncodeToString(sum[:]) != de.Name() {
				fatalError(logger, "issuer file hash mismatch", "file", name)
			}
			zf, err := w.CreateHeader(&zip.FileHeader{
				Name:   name,
				Method: zip.Deflate,
			})
			if err != nil {
				fatalError(logger, "failed to create zip entry", "file", name, "err", err)
			}
			if _, err := zf.Write(data); err != nil {
				fatalError(logger, "failed to write zip entry", "file", name, "err", err)
			}
		}

		// Store high-level and partial tiles.
		for _, tile := range globalTiles {
			if err := storeTile(w, tile, hr); err != nil {
				fatalError(logger, "failed to store tile", "tile", sunlight.TilePath(tile), "err", err)
			}
		}

		// Store sub-tree tiles, starting from higher-level ones.
		tiles := tlog.NewTiles(torchwood.TileHeight, n, n+subtree)
		pb := progressbar.Default(int64(len(tiles)), name)
		slices.SortStableFunc(tiles, func(a, b tlog.Tile) int {
			switch {
			case a.L < b.L:
				return 1
			case a.L > b.L:
				return -1
			default:
				return 0
			}
		})
		for _, tile := range tiles {
			if tile.L >= 3 || tile.W < sunlight.TileWidth {
				// Already stored as part of globalTiles.
				pb.Add(1)
				continue
			}
			if err := storeTile(w, tile, hr); err != nil {
				fatalError(logger, "failed to store tile", "tile", sunlight.TilePath(tile), "err", err)
			}
			pb.Add(1)
			if err := ctx.Err(); err != nil {
				fatalError(logger, "interrupted", "err", err)
			}
		}
		pb.Reset()
		pb.ChangeMax64((subtree + 255) / 256)

		// Verify and store data tiles after the Merkle tree tiles.
		for _, tile := range tiles {
			if tile.L != 0 {
				continue
			}
			tile.L = -1
			path := sunlight.TilePath(tile)
			data, err := root.ReadFile(path)
			if err != nil {
				fatalError(logger, "failed to read tile data", "tile", path, "err", err)
			}
			if err := verifyTileData(tile, data, hr); err != nil {
				fatalError(logger, "failed to verify tile data", "tile", path, "err", err)
			}
			zf, err := w.CreateHeader(&zip.FileHeader{
				Name:   path,
				Method: zip.Deflate,
			})
			if err != nil {
				fatalError(logger, "failed to create zip entry", "tile", path, "err", err)
			}
			if _, err := zf.Write(data); err != nil {
				fatalError(logger, "failed to write zip entry", "tile", path, "err", err)
			}
			pb.Add(1)
			if err := ctx.Err(); err != nil {
				fatalError(logger, "interrupted", "err", err)
			}
		}
		if err := w.Close(); err != nil {
			fatalError(logger, "failed to finalize zip file", "name", name, "err", err)
		}
		if err := f.Close(); err != nil {
			fatalError(logger, "failed to close zip file", "name", name, "err", err)
		}
		pb.Exit()
		if err := root.Rename(name+".tmp", name); err != nil {
			fatalError(logger, "failed to rename zip file", "name", name, "err", err)
		}
		logger.Info("wrote zip file", "name", name)
	}

	logger.Info("done")
}

func storeMetadataFile(w *zip.Writer, name string, data []byte) error {
	zf, err := w.CreateHeader(&zip.FileHeader{
		Name:   name,
		Method: zip.Store,
	})
	if err != nil {
		return fmt.Errorf("failed to create zip entry %q: %w", name, err)
	}
	if _, err := zf.Write(data); err != nil {
		return fmt.Errorf("failed to write zip entry %q: %w", name, err)
	}
	return nil
}

func storeTile(w *zip.Writer, tile tlog.Tile, hr tlog.HashReader) error {
	path := sunlight.TilePath(tile)
	// Pull the hashes through TileHashReader instead of reading them
	// directly, so that their inclusion in the tree is verified.
	data, err := tlog.ReadTileData(tile, hr)
	if err != nil {
		return fmt.Errorf("failed to read tile data %q: %w", path, err)
	}
	zf, err := w.CreateHeader(&zip.FileHeader{
		Name:   path,
		Method: zip.Store, // hashes don't compress!
	})
	if err != nil {
		return fmt.Errorf("failed to create zip entry %q: %w", path, err)
	}
	if _, err := zf.Write(data); err != nil {
		return fmt.Errorf("failed to write zip entry %q: %w", path, err)
	}
	return nil
}

func verifyTileData(tile tlog.Tile, data []byte, hr tlog.HashReader) error {
	if tile.L != -1 {
		return fmt.Errorf("not a data tile")
	}
	indexes := make([]int64, 0, tile.W)
	for i := range tile.W {
		indexes = append(indexes, tlog.StoredHashIndex(0, tile.N*256+int64(i)))
	}
	hashes, err := hr.ReadHashes(indexes)
	if err != nil {
		return fmt.Errorf("failed to read record hashes: %w", err)
	}
	for i, h := range hashes {
		var e *sunlight.LogEntry
		e, data, err = sunlight.ReadTileLeafMaybeArchival(data)
		if err != nil {
			return fmt.Errorf("failed to read tile leaf: %w", err)
		}
		if !e.RFC6962ArchivalLeaf && e.LeafIndex != tile.N*256+int64(i) {
			return fmt.Errorf("unexpected leaf index %d, want %d", e.LeafIndex, tile.N*256+int64(i))
		}
		if rh := tlog.RecordHash(e.MerkleTreeLeaf()); rh != h {
			return fmt.Errorf("record hash mismatch at index %d", tile.N*256+int64(i))
		}
	}
	if len(data) != 0 {
		return fmt.Errorf("trailing data")
	}
	return nil
}

type localTileReader struct{ fs.FS }

func (l localTileReader) ReadTiles(ctx context.Context, tiles []tlog.Tile) (data [][]byte, err error) {
	data = make([][]byte, len(tiles))
	for i, tile := range tiles {
		d, err := fs.ReadFile(l.FS, sunlight.TilePath(tile))
		if err != nil {
			return nil, err
		}
		data[i] = d
	}
	return data, nil
}

func (l localTileReader) SaveTiles(tiles []tlog.Tile, data [][]byte) {}

func (l localTileReader) ReadEndpoint(ctx context.Context, path string) ([]byte, error) {
	return fs.ReadFile(l.FS, path)
}

func fatalError(logger *slog.Logger, msg string, args ...any) {
	logger.Error(msg, args...)
	os.Exit(1)
}
